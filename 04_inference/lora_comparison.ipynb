{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "896647b1",
   "metadata": {},
   "source": [
    "# LoRA 모델 성능 비교 분석\n",
    "\n",
    "이 노트북은 베이스 모델과 LoRA 가중치를 로딩한 모델의 성능을 비교하여 시각적으로 확인할 수 있습니다.\n",
    "\n",
    "## 기능\n",
    "- 베이스 모델 (SD/SDXL) 이미지 생성\n",
    "- LoRA 가중치를 로딩한 모델 이미지 생성\n",
    "- 두 모델의 결과를 나란히 비교\n",
    "- 다양한 프롬프트로 성능 테스트\n",
    "- 파라미터 조정 실험"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6525ae8f",
   "metadata": {},
   "source": [
    "# 1. 준비 작업"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7be9c369",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda/envs/m_train/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/ubuntu/miniconda/envs/m_train/lib/python3.10/site-packages/torch/cuda/__init__.py:174: UserWarning: CUDA initialization: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 802: system not yet initialized (Triggered internally at /pytorch/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n",
      "The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers and GPU quantization are unavailable.\n"
     ]
    }
   ],
   "source": [
    "# 필요한 라이브러리 import\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from enum import Enum\n",
    "from typing import Any, Dict, Final, List, Tuple\n",
    "from diffusers import StableDiffusionPipeline, DiffusionPipeline, EulerDiscreteScheduler\n",
    "from diffusers.models import AutoencoderKL\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 한글 폰트 설정 (선택사항)\n",
    "plt.rcParams['font.family'] = ['DejaVu Sans']\n",
    "plt.rcParams['axes.unicode_minus'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e5f05a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 ID 정의 (test_lora_local.py와 동일)\n",
    "class HfModelId(str, Enum):\n",
    "    # SD 모델\n",
    "    SD_V1_5: str = \"SG161222/Realistic_Vision_V5.1_noVAE\"\n",
    "    SD_VAE: str = \"stabilityai/sd-vae-ft-mse\"\n",
    "    \n",
    "    # SDXL 모델\n",
    "    SDXL_V1_0_BASE: str = \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
    "    SDXL_V1_0_REFINER: str = \"stabilityai/stable-diffusion-xl-refiner-1.0\"\n",
    "\n",
    "ENABLE_MODEL_CPU_OFFLOAD: Final = True\n",
    "USE_REFINER: Final = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1db19df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SD 모델 로딩 함수들\n",
    "def load_sd_base_model() -> Any:\n",
    "    \"\"\"SD 베이스 모델 로딩 (LoRA 없이)\"\"\"\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"Loading SD base model on device: {device}\")\n",
    "\n",
    "    model = StableDiffusionPipeline.from_pretrained(\n",
    "        HfModelId.SD_V1_5.value, torch_dtype=torch.float16\n",
    "    ).to(device)\n",
    "\n",
    "    model.vae = AutoencoderKL.from_pretrained(\n",
    "        HfModelId.SD_VAE.value, torch_dtype=torch.float16\n",
    "    ).to(device)\n",
    "\n",
    "    model.scheduler = EulerDiscreteScheduler.from_config(\n",
    "        model.scheduler.config, use_karras_sigmas=True\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "def load_sd_lora_model(model_dir: str) -> Any:\n",
    "    \"\"\"SD LoRA 모델 로딩 (베이스 + LoRA)\"\"\"\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"Loading SD LoRA model on device: {device}\")\n",
    "\n",
    "    model = StableDiffusionPipeline.from_pretrained(\n",
    "        HfModelId.SD_V1_5.value, torch_dtype=torch.float16\n",
    "    ).to(device)\n",
    "\n",
    "    model.vae = AutoencoderKL.from_pretrained(\n",
    "        HfModelId.SD_VAE.value, torch_dtype=torch.float16\n",
    "    ).to(device)\n",
    "\n",
    "    model.scheduler = EulerDiscreteScheduler.from_config(\n",
    "        model.scheduler.config, use_karras_sigmas=True\n",
    "    )\n",
    "\n",
    "    print(f\"Loading LoRA weights from: {model_dir}\")\n",
    "    model.load_lora_weights(model_dir)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0895029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SDXL 모델 로딩 함수들\n",
    "def load_sdxl_base_model() -> Dict[str, Any]:\n",
    "    \"\"\"SDXL 베이스 모델 로딩 (LoRA 없이)\"\"\"\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"Loading SDXL base model on device: {device}\")\n",
    "\n",
    "    model = DiffusionPipeline.from_pretrained(\n",
    "        HfModelId.SDXL_V1_0_BASE.value,\n",
    "        torch_dtype=torch.float16,\n",
    "        variant=\"fp16\",\n",
    "    )\n",
    "    \n",
    "    _ = (\n",
    "        model.enable_model_cpu_offload()\n",
    "        if ENABLE_MODEL_CPU_OFFLOAD\n",
    "        else model.to(device)\n",
    "    )\n",
    "\n",
    "    return {\"model\": model, \"refiner\": None}\n",
    "\n",
    "def load_sdxl_lora_model(model_dir: str) -> Dict[str, Any]:\n",
    "    \"\"\"SDXL LoRA 모델 로딩 (베이스 + LoRA)\"\"\"\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"Loading SDXL LoRA model on device: {device}\")\n",
    "\n",
    "    model = DiffusionPipeline.from_pretrained(\n",
    "        HfModelId.SDXL_V1_0_BASE.value,\n",
    "        torch_dtype=torch.float16,\n",
    "        variant=\"fp16\",\n",
    "    )\n",
    "    \n",
    "    _ = (\n",
    "        model.enable_model_cpu_offload()\n",
    "        if ENABLE_MODEL_CPU_OFFLOAD\n",
    "        else model.to(device)\n",
    "    )\n",
    "\n",
    "    print(f\"Loading LoRA weights from: {model_dir}\")\n",
    "    model.load_lora_weights(model_dir)\n",
    "\n",
    "    return {\"model\": model, \"refiner\": None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd4bbb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지 생성 함수들\n",
    "def generate_sd_image(model: Any, prompt: str, **kwargs) -> Image.Image:\n",
    "    \"\"\"SD 모델로 이미지 생성\"\"\"\n",
    "    height = kwargs.get(\"height\", 512)\n",
    "    width = kwargs.get(\"width\", 512)\n",
    "    num_inference_steps = kwargs.get(\"num_inference_steps\", 50)\n",
    "    guidance_scale = kwargs.get(\"guidance_scale\", 7.5)\n",
    "    negative_prompt = kwargs.get(\"negative_prompt\", None)\n",
    "    num_images_per_prompt = kwargs.get(\"num_images_per_prompt\", 1)\n",
    "    seed = kwargs.get(\"seed\", 42)\n",
    "    cross_attention_scale = kwargs.get(\"cross_attention_scale\", 0.5)\n",
    "\n",
    "    negative_prompt = negative_prompt if negative_prompt and len(negative_prompt) > 0 else None\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    generator = torch.Generator(device=device).manual_seed(seed)\n",
    "    generated_images = model(\n",
    "        prompt,\n",
    "        height=height,\n",
    "        width=width,\n",
    "        num_inference_steps=num_inference_steps,\n",
    "        guidance_scale=guidance_scale,\n",
    "        negative_prompt=negative_prompt,\n",
    "        num_images_per_prompt=num_images_per_prompt,\n",
    "        generator=generator,\n",
    "        cross_attention_kwargs={\"scale\": cross_attention_scale},\n",
    "    )[\"images\"]\n",
    "\n",
    "    return generated_images[0]\n",
    "\n",
    "def generate_sdxl_image(model_dict: Dict[str, Any], prompt: str, **kwargs) -> Image.Image:\n",
    "    \"\"\"SDXL 모델로 이미지 생성\"\"\"\n",
    "    height = kwargs.get(\"height\", 1024)\n",
    "    width = kwargs.get(\"width\", 1024)\n",
    "    num_inference_steps = kwargs.get(\"num_inference_steps\", 50)\n",
    "    guidance_scale = kwargs.get(\"guidance_scale\", 7.5)\n",
    "    negative_prompt = kwargs.get(\"negative_prompt\", None)\n",
    "    num_images_per_prompt = kwargs.get(\"num_images_per_prompt\", 1)\n",
    "    seed = kwargs.get(\"seed\", 42)\n",
    "    high_noise_frac = kwargs.get(\"high_noise_frac\", 0.7)\n",
    "    cross_attention_scale = kwargs.get(\"cross_attention_scale\", 0.5)\n",
    "\n",
    "    negative_prompt = negative_prompt if negative_prompt and len(negative_prompt) > 0 else None\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    model, refiner = model_dict[\"model\"], model_dict[\"refiner\"]\n",
    "    generator = torch.Generator(device=device).manual_seed(seed)\n",
    "\n",
    "    if USE_REFINER and refiner:\n",
    "        image = model(\n",
    "            prompt=prompt,\n",
    "            height=height,\n",
    "            width=width,\n",
    "            num_inference_steps=num_inference_steps,\n",
    "            guidance_scale=guidance_scale,\n",
    "            negative_prompt=negative_prompt,\n",
    "            num_images_per_prompt=num_images_per_prompt,\n",
    "            denoising_end=high_noise_frac,\n",
    "            generator=generator,\n",
    "            output_type=\"latent\",\n",
    "            cross_attention_kwargs={\"scale\": cross_attention_scale},\n",
    "        )[\"images\"]\n",
    "        generated_images = refiner(\n",
    "            prompt=prompt,\n",
    "            image=image,\n",
    "            num_inference_steps=num_inference_steps,\n",
    "            denoising_start=high_noise_frac,\n",
    "        )[\"images\"]\n",
    "    else:\n",
    "        generated_images = model(\n",
    "            prompt=prompt,\n",
    "            height=height,\n",
    "            width=width,\n",
    "            num_inference_steps=num_inference_steps,\n",
    "            guidance_scale=guidance_scale,\n",
    "            negative_prompt=negative_prompt,\n",
    "            num_images_per_prompt=num_images_per_prompt,\n",
    "            generator=generator,\n",
    "            cross_attention_kwargs={\"scale\": cross_attention_scale},\n",
    "        )[\"images\"]\n",
    "\n",
    "    return generated_images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d26091d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지 비교 시각화 함수\n",
    "def compare_images(base_image: Image.Image, lora_image: Image.Image, \n",
    "                  prompt: str, model_type: str = \"SDXL\", \n",
    "                  figsize: Tuple[int, int] = (12, 6)):\n",
    "    \"\"\"베이스 모델과 LoRA 모델의 이미지를 나란히 비교하여 표시\"\"\"\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=figsize)\n",
    "    \n",
    "    # 베이스 모델 이미지\n",
    "    ax1.imshow(base_image)\n",
    "    ax1.set_title(f'{model_type} Base Model\\n{prompt[:50]}{\"...\" if len(prompt) > 50 else \"\"}', \n",
    "                  fontsize=12, fontweight='bold')\n",
    "    ax1.axis('off')\n",
    "    \n",
    "    # LoRA 모델 이미지\n",
    "    ax2.imshow(lora_image)\n",
    "    ax2.set_title(f'{model_type} LoRA Model\\n{prompt[:50]}{\"...\" if len(prompt) > 50 else \"\"}', \n",
    "                  fontsize=12, fontweight='bold')\n",
    "    ax2.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 이미지 정보 출력\n",
    "    print(f\"\\n=== 이미지 비교 결과 ===\")\n",
    "    print(f\"프롬프트: {prompt}\")\n",
    "    print(f\"베이스 모델 이미지 크기: {base_image.size}\")\n",
    "    print(f\"LoRA 모델 이미지 크기: {lora_image.size}\")\n",
    "    print(f\"모델 타입: {model_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0e09093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델 타입: SDXL\n",
      "LoRA 경로: ../models/\n",
      "CUDA 사용 가능: False\n"
     ]
    }
   ],
   "source": [
    "# 설정\n",
    "MODEL_TYPE = \"SDXL\"  # \"SD\" 또는 \"SDXL\"\n",
    "LORA_PATH = \"../models/\"  # LoRA 가중치 경로\n",
    "\n",
    "print(f\"모델 타입: {MODEL_TYPE}\")\n",
    "print(f\"LoRA 경로: {LORA_PATH}\")\n",
    "print(f\"CUDA 사용 가능: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU 메모리: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "888904e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 모델 로딩 시작 ===\n",
      "\n",
      "1. SDXL 베이스 모델 로딩 중...\n",
      "Loading SDXL base model on device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...: 100%|██████████| 7/7 [00:02<00:00,  2.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2. SDXL LoRA 모델 로딩 중...\n",
      "Loading SDXL LoRA model on device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...: 100%|██████████| 7/7 [00:00<00:00,  8.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LoRA weights from: ../models/\n",
      "\n",
      "=== 모델 로딩 완료 ===\n"
     ]
    }
   ],
   "source": [
    "# 모델 로딩\n",
    "print(\"=== 모델 로딩 시작 ===\")\n",
    "\n",
    "if MODEL_TYPE == \"SD\":\n",
    "    # SD 모델 로딩\n",
    "    print(\"\\n1. SD 베이스 모델 로딩 중...\")\n",
    "    sd_base_model = load_sd_base_model()\n",
    "    \n",
    "    print(\"\\n2. SD LoRA 모델 로딩 중...\")\n",
    "    sd_lora_model = load_sd_lora_model(LORA_PATH)\n",
    "    \n",
    "    base_model = sd_base_model\n",
    "    lora_model = sd_lora_model\n",
    "    \n",
    "else:\n",
    "    # SDXL 모델 로딩\n",
    "    print(\"\\n1. SDXL 베이스 모델 로딩 중...\")\n",
    "    sdxl_base_model = load_sdxl_base_model()\n",
    "    \n",
    "    print(\"\\n2. SDXL LoRA 모델 로딩 중...\")\n",
    "    sdxl_lora_model = load_sdxl_lora_model(LORA_PATH)\n",
    "    \n",
    "    base_model = sdxl_base_model\n",
    "    lora_model = sdxl_lora_model\n",
    "\n",
    "print(\"\\n=== 모델 로딩 완료 ===\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f800e2bc",
   "metadata": {},
   "source": [
    "# 2. 실험 시작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53ec4900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테스트 파라미터: {'num_inference_steps': 30, 'guidance_scale': 7.5, 'seed': 42, 'cross_attention_scale': 0.5, 'height': 1024, 'width': 1024}\n"
     ]
    }
   ],
   "source": [
    "# 테스트 프롬프트\n",
    "test_prompts = [\n",
    "    # 컨셉: 캐주얼 스트릿 스타일\n",
    "    # 한국어: 오버사이즈 크림 후드티와 와이드 진으로 편안한 캐주얼 룩, 서울 쇼핑가 배경\n",
    "    \"Young Korean woman wearing oversized cream hoodie, wide-leg denim pants, chunky white sneakers, baseball cap, crossbody bag. Casual street style in Seoul shopping district.\",\n",
    "    \n",
    "    # 컨셉: 미니멀 오피스 룩\n",
    "    # 한국어: 테일러드 베이지 재킷과 스트레이트 팬츠로 완성한 미니멀 오피스 룩\n",
    "    \"Korean businesswoman in tailored beige blazer, white button shirt, black straight trousers, pointed-toe flats, structured handbag. Professional minimal aesthetic, office building background.\",\n",
    "    \n",
    "    # 컨셉: 빈티지 레트로 스타일\n",
    "    # 한국어: 70년대 빈티지 감성의 레트로 스타일, 플레어 진과 플랫폼 부츠 조합\n",
    "    \"Young Korean woman wearing vintage brown leather jacket, high-waisted flare jeans, platform boots, retro sunglasses, vintage band t-shirt. 70s inspired fashion, urban cafe setting.\",\n",
    "    \n",
    "    # 컨셉: 로맨틱 페미닌 룩\n",
    "    # 한국어: 미디 스커트와 핑크 가디건으로 완성한 로맨틱 페미닌 룩, 벚꽃 공원 배경\n",
    "    \"Korean woman in flowing midi skirt, soft pink cardigan, white blouse, mary-jane heels, pearl accessories. Romantic feminine style in cherry blossom park setting.\",\n",
    "    \n",
    "    # 컨셉: 스포티 애슬레저\n",
    "    # 한국어: 크롭 스포츠 재킷과 레깅스로 완성한 애슬레저 스타일\n",
    "    \"Young Korean woman wearing cropped sports jacket, high-waist leggings, chunky dad sneakers, baseball cap, fanny pack. Athletic streetwear style, urban gym exterior.\",\n",
    "    \n",
    "    # 컨셉: 모던 한복 퓨전\n",
    "    # 한국어: 현대적으로 재해석된 한복 퓨전 스타일, 크롭 저고리와 와이드 팬츠\n",
    "    \"Korean woman in modern hanbok-inspired outfit, cropped jeogori top, wide palazzo pants, traditional hair accessories, modern sneakers. Contemporary Korean fashion fusion style.\",\n",
    "    \n",
    "    # 컨셉: 그런지 에지 스타일\n",
    "    # 한국어: 찢어진 데님과 플래널 셔츠로 완성한 그런지 에지 스타일\n",
    "    \"Young Korean woman wearing distressed denim jacket, plaid flannel shirt, ripped black jeans, combat boots, chain accessories. Grunge aesthetic in underground music venue.\",\n",
    "    \n",
    "    # 컨셉: 프레피 스쿨 룩\n",
    "    # 한국어: 네이비 블레이저와 플리츠 스커트로 완성한 프레피 스쿨 룩\n",
    "    \"Korean student wearing navy blazer, white collar shirt, pleated skirt, knee-high socks, loafers, backpack. Preppy school uniform style, university campus background.\",\n",
    "    \n",
    "    # 컨셉: 보헤미안 시크\n",
    "    # 한국어: 맥시 드레스와 데님 재킷으로 완성한 보헤미안 시크 스타일\n",
    "    \"Young Korean woman in flowing maxi dress, denim jacket, ankle boots, layered jewelry, fringe bag, wide-brim hat. Bohemian chic style in outdoor music festival.\",\n",
    "    \n",
    "    # 컨셉: 미니멀 모노톤\n",
    "    # 한국어: 터틀넥과 울 코트로 완성한 모노톤 미니멀 스타일\n",
    "    \"Korean woman wearing black turtleneck, grey wool coat, black skinny pants, white minimalist sneakers, structured tote bag. Monochrome minimalist fashion, modern architecture background.\"\n",
    "]\n",
    "\n",
    "# 기본 파라미터\n",
    "params = {\n",
    "    \"num_inference_steps\": 30,  # 빠른 테스트를 위해 줄임\n",
    "    \"guidance_scale\": 7.5,\n",
    "    \"seed\": 42,\n",
    "    \"cross_attention_scale\": 0.5\n",
    "}\n",
    "\n",
    "if MODEL_TYPE == \"SD\":\n",
    "    params.update({\"height\": 512, \"width\": 512})\n",
    "else:\n",
    "    params.update({\"height\": 1024, \"width\": 1024})\n",
    "\n",
    "print(f\"테스트 파라미터: {params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7bb77c",
   "metadata": {},
   "source": [
    "## 첫번째 프롬프트"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a192fe24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 테스트 프롬프트: Young Korean woman wearing oversized cream hoodie, wide-leg denim pants, chunky white sneakers, baseball cap, crossbody bag. Casual street style in Seoul shopping district. ===\n",
      "\n",
      "1. 베이스 모델로 이미지 생성 중...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 802: system not yet initialized",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m     base_image \u001b[38;5;241m=\u001b[39m generate_sd_image(base_model, prompt, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 10\u001b[0m     base_image \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_sdxl_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# LoRA 모델로 이미지 생성\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m2. LoRA 모델로 이미지 생성 중...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[5], line 70\u001b[0m, in \u001b[0;36mgenerate_sdxl_image\u001b[0;34m(model_dict, prompt, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m     generated_images \u001b[38;5;241m=\u001b[39m refiner(\n\u001b[1;32m     64\u001b[0m         prompt\u001b[38;5;241m=\u001b[39mprompt,\n\u001b[1;32m     65\u001b[0m         image\u001b[38;5;241m=\u001b[39mimage,\n\u001b[1;32m     66\u001b[0m         num_inference_steps\u001b[38;5;241m=\u001b[39mnum_inference_steps,\n\u001b[1;32m     67\u001b[0m         denoising_start\u001b[38;5;241m=\u001b[39mhigh_noise_frac,\n\u001b[1;32m     68\u001b[0m     )[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 70\u001b[0m     generated_images \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwidth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwidth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_inference_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_inference_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m        \u001b[49m\u001b[43mguidance_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mguidance_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnegative_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnegative_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_images_per_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_images_per_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcross_attention_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mscale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcross_attention_scale\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m generated_images[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda/envs/m_train/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda/envs/m_train/lib/python3.10/site-packages/diffusers/pipelines/stable_diffusion_xl/pipeline_stable_diffusion_xl.py:1094\u001b[0m, in \u001b[0;36mStableDiffusionXLPipeline.__call__\u001b[0;34m(self, prompt, prompt_2, height, width, num_inference_steps, timesteps, sigmas, denoising_end, guidance_scale, negative_prompt, negative_prompt_2, num_images_per_prompt, eta, generator, latents, prompt_embeds, negative_prompt_embeds, pooled_prompt_embeds, negative_pooled_prompt_embeds, ip_adapter_image, ip_adapter_image_embeds, output_type, return_dict, cross_attention_kwargs, guidance_rescale, original_size, crops_coords_top_left, target_size, negative_original_size, negative_crops_coords_top_left, negative_target_size, clip_skip, callback_on_step_end, callback_on_step_end_tensor_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1084\u001b[0m \u001b[38;5;66;03m# 3. Encode input prompt\u001b[39;00m\n\u001b[1;32m   1085\u001b[0m lora_scale \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1086\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcross_attention_kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscale\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcross_attention_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1087\u001b[0m )\n\u001b[1;32m   1089\u001b[0m (\n\u001b[1;32m   1090\u001b[0m     prompt_embeds,\n\u001b[1;32m   1091\u001b[0m     negative_prompt_embeds,\n\u001b[1;32m   1092\u001b[0m     pooled_prompt_embeds,\n\u001b[1;32m   1093\u001b[0m     negative_pooled_prompt_embeds,\n\u001b[0;32m-> 1094\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1095\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1096\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt_2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt_2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1097\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1098\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_images_per_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_images_per_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1099\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_classifier_free_guidance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_classifier_free_guidance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1100\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnegative_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnegative_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1101\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnegative_prompt_2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnegative_prompt_2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1102\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnegative_prompt_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnegative_prompt_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1104\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpooled_prompt_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpooled_prompt_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1105\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnegative_pooled_prompt_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnegative_pooled_prompt_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1106\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlora_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlora_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1107\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclip_skip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclip_skip\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1108\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1110\u001b[0m \u001b[38;5;66;03m# 4. Prepare timesteps\u001b[39;00m\n\u001b[1;32m   1111\u001b[0m timesteps, num_inference_steps \u001b[38;5;241m=\u001b[39m retrieve_timesteps(\n\u001b[1;32m   1112\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscheduler, num_inference_steps, device, timesteps, sigmas\n\u001b[1;32m   1113\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda/envs/m_train/lib/python3.10/site-packages/diffusers/pipelines/stable_diffusion_xl/pipeline_stable_diffusion_xl.py:410\u001b[0m, in \u001b[0;36mStableDiffusionXLPipeline.encode_prompt\u001b[0;34m(self, prompt, prompt_2, device, num_images_per_prompt, do_classifier_free_guidance, negative_prompt, negative_prompt_2, prompt_embeds, negative_prompt_embeds, pooled_prompt_embeds, negative_pooled_prompt_embeds, lora_scale, clip_skip)\u001b[0m\n\u001b[1;32m    404\u001b[0m     removed_text \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(untruncated_ids[:, tokenizer\u001b[38;5;241m.\u001b[39mmodel_max_length \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m : \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    405\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe following part of your input was truncated because CLIP can only handle sequences up to\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    407\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtokenizer\u001b[38;5;241m.\u001b[39mmodel_max_length\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m tokens: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mremoved_text\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    408\u001b[0m     )\n\u001b[0;32m--> 410\u001b[0m prompt_embeds \u001b[38;5;241m=\u001b[39m text_encoder(\u001b[43mtext_input_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m, output_hidden_states\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    412\u001b[0m \u001b[38;5;66;03m# We are only ALWAYS interested in the pooled output of the final text encoder\u001b[39;00m\n\u001b[1;32m    413\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pooled_prompt_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m prompt_embeds[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda/envs/m_train/lib/python3.10/site-packages/torch/cuda/__init__.py:372\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron:\n\u001b[1;32m    371\u001b[0m     os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLAZY\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 372\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;66;03m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;66;03m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;66;03m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[1;32m    376\u001b[0m _tls\u001b[38;5;241m.\u001b[39mis_initializing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 802: system not yet initialized"
     ]
    }
   ],
   "source": [
    "# 첫 번째 프롬프트로 테스트\n",
    "prompt = test_prompts[0]\n",
    "print(f\"\\n=== 테스트 프롬프트: {prompt} ===\")\n",
    "\n",
    "# 베이스 모델로 이미지 생성\n",
    "print(\"\\n1. 베이스 모델로 이미지 생성 중...\")\n",
    "if MODEL_TYPE == \"SD\":\n",
    "    base_image = generate_sd_image(base_model, prompt, **params)\n",
    "else:\n",
    "    base_image = generate_sdxl_image(base_model, prompt, **params)\n",
    "\n",
    "# LoRA 모델로 이미지 생성\n",
    "print(\"\\n2. LoRA 모델로 이미지 생성 중...\")\n",
    "if MODEL_TYPE == \"SD\":\n",
    "    lora_image = generate_sd_image(lora_model, prompt, **params)\n",
    "else:\n",
    "    lora_image = generate_sdxl_image(lora_model, prompt, **params)\n",
    "\n",
    "# 결과 비교\n",
    "compare_images(base_image, lora_image, prompt, MODEL_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98452db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여러 프롬프트로 테스트\n",
    "for i, prompt in enumerate(test_prompts[1:], 1):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"테스트 {i+1}: {prompt}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # 베이스 모델로 이미지 생성\n",
    "    print(\"베이스 모델로 이미지 생성 중...\")\n",
    "    if MODEL_TYPE == \"SD\":\n",
    "        base_image = generate_sd_image(base_model, prompt, **params)\n",
    "    else:\n",
    "        base_image = generate_sdxl_image(base_model, prompt, **params)\n",
    "\n",
    "    # LoRA 모델로 이미지 생성\n",
    "    print(\"LoRA 모델로 이미지 생성 중...\")\n",
    "    if MODEL_TYPE == \"SD\":\n",
    "        lora_image = generate_sd_image(lora_model, prompt, **params)\n",
    "    else:\n",
    "        lora_image = generate_sdxl_image(lora_model, prompt, **params)\n",
    "\n",
    "    # 결과 비교\n",
    "    compare_images(base_image, lora_image, prompt, MODEL_TYPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a8947c",
   "metadata": {},
   "source": [
    "## 사용자 정의 프롬프트\n",
    "- Negatve 반영"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "92805fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사용자 정의 프롬프트 테스트 함수\n",
    "def test_custom_prompt(prompt: str, negative_prompt: str = \"\", \n",
    "                      guidance_scale: float = 7.5, \n",
    "                      cross_attention_scale: float = 0.5):\n",
    "    \"\"\"사용자 정의 프롬프트로 베이스 모델과 LoRA 모델 비교\"\"\"\n",
    "    \n",
    "    print(f\"\\n=== 사용자 정의 프롬프트 테스트 ===\")\n",
    "    print(f\"프롬프트: {prompt}\")\n",
    "    print(f\"네거티브 프롬프트: {negative_prompt if negative_prompt else '(없음)'}\")\n",
    "    \n",
    "    # 파라미터 설정\n",
    "    test_params = params.copy()\n",
    "    test_params.update({\n",
    "        \"guidance_scale\": guidance_scale,\n",
    "        \"cross_attention_scale\": cross_attention_scale,\n",
    "        \"negative_prompt\": negative_prompt\n",
    "    })\n",
    "    \n",
    "    # 베이스 모델로 이미지 생성\n",
    "    print(\"\\n1. 베이스 모델로 이미지 생성 중...\")\n",
    "    if MODEL_TYPE == \"SD\":\n",
    "        base_image = generate_sd_image(base_model, prompt, **test_params)\n",
    "    else:\n",
    "        base_image = generate_sdxl_image(base_model, prompt, **test_params)\n",
    "\n",
    "    # LoRA 모델로 이미지 생성\n",
    "    print(\"\\n2. LoRA 모델로 이미지 생성 중...\")\n",
    "    if MODEL_TYPE == \"SD\":\n",
    "        lora_image = generate_sd_image(lora_model, prompt, **test_params)\n",
    "    else:\n",
    "        lora_image = generate_sdxl_image(lora_model, prompt, **test_params)\n",
    "\n",
    "    # 결과 비교\n",
    "    compare_images(base_image, lora_image, prompt, MODEL_TYPE)\n",
    "    \n",
    "    return base_image, lora_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26dd018b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예시: 사용자 정의 프롬프트 테스트\n",
    "# 아래 프롬프트를 원하는 것으로 변경하여 테스트하세요\n",
    "\n",
    "custom_prompt = \"Young Korean woman with short brown bob hairstyle, wearing oversized gray crewneck sweatshirt with embroidered logo patch, distressed blue denim shorts with frayed hems, white knee-high socks with black stripes, chunky white sneakers with platform sole and colorful laces. Standing casually, hand touching hair. Youthful street style with comfortable oversized fit and vintage-inspired details.\"\n",
    "custom_negative = \"blurry, low quality, distorted\"\n",
    "\n",
    "base_img, lora_img = test_custom_prompt(\n",
    "    prompt=custom_prompt,\n",
    "    negative_prompt=custom_negative,\n",
    "    guidance_scale=8.0,\n",
    "    cross_attention_scale=0.6\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed00149",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11eb17d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "m_train",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
